---
title: "Getting Started with Modeltime H2O"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with Modeltime H2O}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  
  out.width='100%',
  fig.align = "center",
  fig.width = 7,
  fig.height = 5,
  
  message = FALSE,
  warning = FALSE
)
```

<img src="logo.png" width="147" height="170" align="right" />

Forecasting with `modeltime.h2o` made easy! This short tutorial shows how you can use:

- __H2O_AutoML__ for forecasting implemented via `automl_reg()`. This function trains and cross-validates multiple machine learning and deep learning models (XGBoost GBM, GLMs, Random Forest, GBMs...) and then trains two Stacked Ensembled models, one of all the models, and one of only the best models of each kind. Finally, the best model is selected based on a stopping metric. And we take care of all this for you!

- __Save & Load Models__ functionality to ensure the persistance of your models.


## Libraries

Load the following librearies:

```{r}
library(tidymodels)
library(modeltime.h2o)
library(tidyverse)
library(timetk)
```


## Collect data and split into training and test sets

Next, we load the walmart_sales_weekly data containing 7 time series and visualize them using the `timetk::plot_time_series()` function.

```{r}
data_tbl <- walmart_sales_weekly %>%
    select(id, Date, Weekly_Sales)

data_tbl %>% 
  group_by(id) %>% 
  timetk::plot_time_series(.date_var   = Date,
                           .value      = Weekly_Sales,
                           .facet_ncol = 2,
                           .smooth     = F)
```

Then, we separate the data with the `initial_time_split()` function and generate a training dataset and a test one.

```{r}
splits <- timetk::time_series_split(data_tbl, assess = "3 month", cumulative = TRUE)

recipe_spec <- recipe(Weekly_Sales ~ ., data = training(splits)) %>%
    step_timeseries_signature(Date) 

train_tbl <- rsample::training(splits) %>% bake(prep(recipe_spec), .)
test_tbl  <- rsample::testing(splits) %>% bake(prep(recipe_spec), .)
```

## Model specification, training and prediction

In order to correctly use modeltime.h2o, it is necessary to connect to an H2O cluster through the h2o.init() function. You can find more information on how to set up the cluster by typing ?h2o.init or by visiting the [official site](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/starting-h2o.html).

```{r}
h2o.init(
    nthreads = -1,
    ip = 'localhost',
    port = 54321
)
```

Now comes the fun part! We define our model specification with the `automl_reg()` function and pass the arguments through the engine:

```{r}
model_spec <- automl_reg(mode = 'regression') %>%
    parsnip::set_engine(
        engine                      = 'h2o',
         max_runtime_secs           = 30, 
         max_runtime_secs_per_model = 30,
         project_name               = 'project_01',
         nfolds                     = 5,
         max_models                 = 1000,
         exclude_algos              = c("DeepLearning"),
         verbosity                  = NULL,
         seed                       =  786
    ) 

model_spec
```

Next, let's train the model!

```{r, message=FALSE}
model_fitted <- model_spec %>%
    fit(Weekly_Sales ~ ., data = train_tbl)

model_fitted
```

Finally, we predict on the test dataset:

```{r, message=FALSE}
predict(model_fitted, test_tbl)
```


## Modeltime Workflow

Once we have our fitted model, we can follow the modeltime workflow from step 3:

- Add fitted models to a __Model Table__.

- __Calibrate__ the models to a testing set.

- Perform Testing Set Forecast & Accuracy Evaluation.

- __Refit__ the models to Full Dataset & Forecast Forward


## Add fitted models to a Model Table

First, we create the model table:

```{r}
modeltime_tbl <- modeltime_table(
    model_fitted
) 

modeltime_tbl
```

## Calibrate & Testing Set Forecast & Accuracy Evaluation

Next, we calibrate to the testing set and visualize the forecasts:

```{r, message=FALSE}
modeltime_tbl %>%
  modeltime_calibrate(test_tbl) %>%
    modeltime_forecast(
        new_data    = test_tbl,
        actual_data = data_tbl,
        keep_data   = TRUE
    ) %>%
    group_by(id) %>%
    plot_modeltime_forecast(.facet_ncol = 2)
```

## Refit to Full Dataset & Forecast Forward

Before using __refit__ on our dataset, let's prepare our data. We create `data_prepared_tbl` which represents the complete dataset (the union of train and test) with the variables created with the recipe named recipe_spec. Subsequently, we create the dataset `future_prepared_tbl` that represents the dataset with the future data to one year and the required variables.

```{r}
data_prepared_tbl <- bind_rows(train_tbl, test_tbl)

future_tbl <- data_prepared_tbl %>%
    group_by(id) %>%
    future_frame(.length_out = "1 year") %>%
    ungroup()

future_prepared_tbl <- bake(prep(recipe_spec), future_tbl)
```

Finally, we use forecast in our future dataset and visualize the results once we had reffited.

```{r, message=FALSE}
refit_tbl <- modeltime_tbl %>%
    modeltime_refit(data_prepared_tbl)

refit_tbl %>%
    modeltime_forecast(
        new_data    = future_prepared_tbl,
        actual_data = data_prepared_tbl,
        keep_data   = TRUE
    ) %>%
    group_by(id) %>%
    plot_modeltime_forecast(.facet_ncol = 2)
```

We can likely do better than this if we train longer but really good for a quick example!

## Saving and Loading Models

H2O models will need to “serialized” (a fancy word for saved to a directory that contains the recipe for recreating the models). To save the models, use `save_h2o_model()`.

- Provide a directory where you want to save the model.
- This saves the model file in the directory.

```{r}
model_fitted %>% 
  save_h2o_model(path = "../model_fitted", overwrite = TRUE)
```

You can reload the model into R using `load_h2o_model()`.

```{r}
model_h2o <- load_h2o_model(path = "../model_fitted/")
```


## Learning More

<p>
<iframe width="100%" height="450" src="https://www.youtube.com/embed/elQb4VzRINg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen style="box-shadow: 0 0 5px 2px rgba(0, 0, 0, .5);"></iframe>
</p>

_My Talk on High-Performance Time Series Forecasting_

Time series is changing. __Businesses now need 10,000+ time series forecasts every day.__ This is what I call a _High-Performance Time Series Forecasting System (HPTSF)_ - Accurate, Robust, and Scalable Forecasting. 

You need to provide a "High-Performance Time Series Forecasting System" (HPTSF System). __High-Performance Forecasting Systems will save companies MILLIONS of dollars.__ 

I teach how to build a HPTFS System in my __High-Performance Time Series Forecasting Course__. If interested in learning Scalable High-Performance Forecasting Strategies then [take my course](https://university.business-science.io/p/ds4b-203-r-high-performance-time-series-forecasting). You will learn:

- Time Series Machine Learning (cutting-edge) with `Modeltime` - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many more)
- NEW - Deep Learning with `GluonTS` (Competition Winners)
- Time Series Preprocessing, Noise Reduction, & Anomaly Detection
- Feature engineering using lagged variables & external regressors
- Hyperparameter Tuning
- Time series cross-validation
- Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner)
- Scalable Forecasting - Forecast 1000+ time series in parallel
- and more.

<p class="text-center" style="font-size:30px;">
<a href="https://university.business-science.io/p/ds4b-203-r-high-performance-time-series-forecasting">Unlock the High-Performance Time Series Forecasting Course</a>
</p>